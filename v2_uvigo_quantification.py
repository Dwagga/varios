# -*- coding: utf-8 -*-
"""v2_Uvigo-quantification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iWd7T0TKgCz2AxhWvO52DiInUBE34yct
"""

from google.colab import drive
drive.mount('/content/drive')

import csv
import matplotlib
import glob
import os

# TensorFlow y tf.keras
import tensorflow as tf
from tensorflow import keras


# Librerias de ayuda
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import sklearn
from sklearn import preprocessing

# Import necessary modules
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical 

import PIL
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
import itertools
!pip install tensorflow-addons
from tensorflow_addons.optimizers import CyclicalLearningRate
from tensorflow.keras.callbacks import ModelCheckpoint

matplotlib.style.use("seaborn-poster")

print(tf.config.list_physical_devices('GPU'))

"""### Definition of functions"""

def plot_map(df, wavel, title, dim_plot_x, dim_plot_y, y_min, y_max):
  plt.rcParams["figure.figsize"] = (dim_plot_x,dim_plot_y)
  ax = plt.gca()
  ax.set_ylim([y_min, y_max])
  for row in df.iloc[:,:].index:
    y=df.iloc[row,:]
    plt.plot(wavel,y,linewidth=1)
  plt.title(title)
  plt.rcParams["figure.figsize"] = plt.rcParamsDefault["figure.figsize"] #Reseteamos los valores de tamaño del plt
  plt.show()

def average_spectrum(df):
  df2 = pd.DataFrame(df.mean(axis=0)).T
  return df2

def plot_cluster_type(df, label, type_cluster, wavel, title, dim_plot_x, dim_plot_y, y_min, y_max):
  count = 0
  plt.rcParams["figure.figsize"] = (dim_plot_x,dim_plot_y)
  ax = plt.gca()
  ax.set_ylim([y_min, y_max])
  for row in df.iloc[:,:].index:
    if(label[row]==type_cluster):
      y=df.iloc[row,:]
      plt.plot(wavel,y,linewidth=1)
      count = count +1
  plt.title(title+" - Total spectra: "+str(count))
  plt.rcParams["figure.figsize"] = plt.rcParamsDefault["figure.figsize"] #Reseteamos los valores de tamaño del plt
  plt.show()

def plot_outliers_index(df, outliers, wavel, title, dim_plot_x, dim_plot_y, y_min, y_max):
  count = 0
  plt.rcParams["figure.figsize"] = (dim_plot_x,dim_plot_y)
  ax = plt.gca()
  ax.set_ylim([y_min, y_max])
  for row in df.iloc[:,:].index:
    if(outliers[row] == 1):
      count = count +1
      y=df.iloc[row,:]
      plt.plot(wavel,y,linewidth=1)
  plt.title(title+" - number of outliers: "+str(count))
  plt.rcParams["figure.figsize"] = plt.rcParamsDefault["figure.figsize"] #Reseteamos los valores de tamaño del plt
  plt.show()

def from_outliers_index_to_df(df, outliers):
  count = 0
  new_df = pd.DataFrame()
  for row in df.iloc[:,:].index:
    if(outliers[row] == 1):
      new_df[count] = df.iloc[row,:]
      count = count+1
  return new_df.T

data_vio  = pd.read_excel('/content/drive/MyDrive/datasets/UVIGO/DATA_v.1.1.xlsx', sheet_name ='Violacein').T
data_deoxy  = pd.read_excel('/content/drive/MyDrive/datasets/UVIGO/DATA_v.1.1.xlsx', sheet_name ='Deoxy').T

display(data_vio)
display(data_deoxy)

wavelength = data_vio.iloc[0,1:].reset_index(drop=True)
display(wavelength)
display(pd.DataFrame(wavelength).T)

data_vio.drop(['RS'], inplace = True )
data_deoxy.drop(['RS'], inplace = True )

display(data_vio)
display(data_deoxy)



print(data_vio.iloc[:,0:2].groupby([0]).count()) #To check how many samples for concentration
print(data_deoxy.iloc[:,0:2].groupby([0]).count()) #To check how many samples for concentration

target_vio = data_vio.iloc[:,0:1].reset_index(drop=True)
target_deoxy = data_deoxy.iloc[:,0:1].reset_index(drop=True)

final_data_vio = data_vio
final_data_deoxy = data_deoxy

final_data_vio.drop(columns=final_data_vio.columns[0], axis=1, inplace=True)
final_data_deoxy.drop(columns=final_data_deoxy.columns[0], axis=1, inplace=True)

display(target_vio)
display(target_deoxy)

target_vio = target_vio.replace([1.e-5, 5.e-6, 1.e-6, 5.e-7, 1.e-7, 5.e-8, 1.e-8, 5.e-9, 1.e-9, 5.e-10, 1.e-10],[50,45,40,35,30,25,20,15,10,5,0])

final_data_vio = final_data_vio.reset_index(drop=True)
final_data_deoxy = final_data_deoxy.reset_index(drop=True)

final_data_vio.columns = range(final_data_vio.shape[1])
final_data_deoxy.columns = range(final_data_deoxy.shape[1])

display(final_data_vio)
display(final_data_deoxy)



plot_map(final_data_vio, wavelength, "Violacein samples", 22, 6, -1000, 180000)
plot_map(final_data_deoxy, wavelength, "Deoxyviolacein samples", 22, 6, -1000, 180000)



"""###Mostramos los espectro VIO de 1.e-05 y 1.e-06 (Por ejemplo)"""

list_samples_index_1e10 = target_vio.loc[target_vio[0].isin([0])]
#print(list_samples)

Vio_1e10 = final_data_vio.iloc[list_samples_index_1e10.index]
#Vio_1e10.head(3)

df_aux_1e10 = Vio_1e10
df_aux_1e10 = df_aux_1e10.reset_index(drop=True)


list_samples_index_5e10 = target_vio.loc[target_vio[0].isin([5])]
Vio_5e10 = final_data_vio.iloc[list_samples_index_5e10.index]
df_aux_5e10 = Vio_5e10
df_aux_5e10 = df_aux_5e10.reset_index(drop=True)


list_samples_index_1e9 = target_vio.loc[target_vio[0].isin([10])]
Vio_1e9 = final_data_vio.iloc[list_samples_index_1e9.index]
df_aux_1e9 = Vio_1e9
df_aux_1e9 = df_aux_1e9.reset_index(drop=True)


list_samples_index_5e9 = target_vio.loc[target_vio[0].isin([15])]
Vio_5e9 = final_data_vio.iloc[list_samples_index_5e9.index]
df_aux_5e9 = Vio_5e9
df_aux_5e9 = df_aux_5e9.reset_index(drop=True)


list_samples_index_1e8 = target_vio.loc[target_vio[0].isin([20])]
Vio_1e8 = final_data_vio.iloc[list_samples_index_1e8.index]
df_aux_1e8 = Vio_1e8
df_aux_1e8 = df_aux_1e8.reset_index(drop=True)


list_samples_index_5e8 = target_vio.loc[target_vio[0].isin([25])]
Vio_5e8 = final_data_vio.iloc[list_samples_index_5e8.index]
df_aux_5e8 = Vio_5e8
df_aux_5e8 = df_aux_5e8.reset_index(drop=True)



list_samples_index_1e7 = target_vio.loc[target_vio[0].isin([30])]
Vio_1e7 = final_data_vio.iloc[list_samples_index_1e7.index]
df_aux_1e7 = Vio_1e7
df_aux_1e7 = df_aux_1e7.reset_index(drop=True)


list_samples_index_5e7 = target_vio.loc[target_vio[0].isin([35])]
Vio_5e7 = final_data_vio.iloc[list_samples_index_5e7.index]
df_aux_5e7 = Vio_5e7
df_aux_5e7 = df_aux_5e7.reset_index(drop=True)


list_samples_index_1e6 = target_vio.loc[target_vio[0].isin([40])]
Vio_1e6 = final_data_vio.iloc[list_samples_index_1e6.index]
df_aux_1e6 = Vio_1e6
df_aux_1e6 = df_aux_1e6.reset_index(drop=True)


list_samples_index_5e6 = target_vio.loc[target_vio[0].isin([45])]
Vio_5e6 = final_data_vio.iloc[list_samples_index_5e6.index]
df_aux_5e6 = Vio_5e6
df_aux_5e6 = df_aux_5e6.reset_index(drop=True)

#plot_map(df_aux, wavelength, "Violacein 1.e-05 and 1.e-06 samples", 22, 6, -1000, 180000)

plot_map(df_aux_1e10, wavelength, "Violacein 1.e-10 samples", 22, 6, -1000, 10000)
plot_map(df_aux_5e10, wavelength, "Violacein 5.e-10 samples", 22, 6, -1000, 10000)
plot_map(df_aux_1e9, wavelength, "Violacein 1.e-9 samples", 22, 6, -1000, 10000)
plot_map(df_aux_5e9, wavelength, "Violacein 5.e-9 samples", 22, 6, -1000, 10000)
plot_map(df_aux_1e8, wavelength, "Violacein 1.e-8 samples", 22, 6, -1000, 10000)
plot_map(df_aux_5e8, wavelength, "Violacein 5.e-8 samples", 22, 6, -1000, 10000)
plot_map(df_aux_1e7, wavelength, "Violacein 1.e-7 samples", 22, 6, -1000, 10000)
plot_map(df_aux_5e7, wavelength, "Violacein 5.e-7 samples", 22, 6, -1000, 10000)
plot_map(df_aux_1e6, wavelength, "Violacein 1.e-6 samples", 22, 6, -1000, 10000)
plot_map(df_aux_5e6, wavelength, "Violacein 5.e-6 samples", 22, 6, -1000, 10000)



"""#Simplificación del problema: modelo de regresión con los ejemplos de VIOLACEINA"""

# Copied from https://github.com/avanwyk/tensorflow-projects/blob/master/lr-finder/lr_finder.py
# Apache License 2.0

from tensorflow.keras.callbacks import Callback

class LRFinder(Callback):
    """`Callback` that exponentially adjusts the learning rate after each training batch between `start_lr` and
    `end_lr` for a maximum number of batches: `max_step`. The loss and learning rate are recorded at each step allowing
    visually finding a good learning rate as per https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html via
    the `plot` method.
    """

    def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 100, smoothing=0.9):
        super(LRFinder, self).__init__()
        self.start_lr, self.end_lr = start_lr, end_lr
        self.max_steps = max_steps
        self.smoothing = smoothing
        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0
        self.lrs, self.losses = [], []

    def on_train_begin(self, logs=None):
        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0
        self.lrs, self.losses = [], []

    def on_train_batch_begin(self, batch, logs=None):
        self.lr = self.exp_annealing(self.step)
        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)

    def on_train_batch_end(self, batch, logs=None):
        logs = logs or {}
        loss = logs.get('loss')
        step = self.step
        if loss:
            self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss
            smooth_loss = self.avg_loss / (1 - self.smoothing ** (self.step + 1))
            self.losses.append(smooth_loss)
            self.lrs.append(self.lr)

            if step == 0 or loss < self.best_loss:
                self.best_loss = loss

            if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):
                self.model.stop_training = True

        if step == self.max_steps:
            self.model.stop_training = True

        self.step += 1

    def exp_annealing(self, step):
        return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps)

    def plot(self):
        fig, ax = plt.subplots(1, 1)
        ax.set_ylabel('Loss')
        ax.set_xlabel('Learning Rate (log scale)')
        ax.set_xscale('log')
        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))
        ax.plot(self.lrs, self.losses)


def pretty_plot(history, field, fn):
  def plot(data, val_data, best_index, best_value, title):
    plt.plot(range(1, len(data)+1), data, label='train')
    plt.plot(range(1, len(data)+1), val_data, label='validation')
    if not best_index is None:
      plt.axvline(x=best_index+1, linestyle=':', c="#777777")
    if not best_value is None:
      plt.axhline(y=best_value, linestyle=':', c="#777777")
    plt.xlabel('Epoch')
    plt.ylabel(field)
    plt.xticks(range(0, len(data), 20))
    plt.title(title)
    plt.legend()
    plt.show()

  data = history.history[field]
  val_data = history.history['val_' + field]
  tail = int(0.15 * len(data))

  best_index = fn(val_data)
  best_value = val_data[best_index]

  plot(data, val_data, best_index, best_value, "{} over epochs (best {:06.4f})".format(field, best_value))
  plot(data[-tail:], val_data[-tail:], None, best_value, "{} over last {} epochs".format(field, tail))

"""##Creamos el set de entrenamiento y de evaluación

###Separamos 2 tipos de muestras aleatorias
"""

import random

lista_tipos_vio = target_vio[0].unique()
lista_tipos_deoxy = target_deoxy[0].unique()

#random.shuffle(lista_tipos_vio)
#random.shuffle(lista_tipos_deoxy)

lista_tipos_vio[0], lista_tipos_vio[6] = lista_tipos_vio[6], lista_tipos_vio[0]

print(lista_tipos_vio)

lista_tipos_vio[2:12]

tipo_1_selec = lista_tipos_vio[0]
tipo_2_selec = lista_tipos_vio[1]


list_samples_index_ts = target_vio.loc[target_vio[0].isin([tipo_1_selec, tipo_2_selec])]
test_data = final_data_vio.iloc[list_samples_index_ts.index]

list_samples_index_tr = target_vio.loc[target_vio[0].isin(lista_tipos_vio[2:12])]
train_data = final_data_vio.iloc[list_samples_index_tr.index]

#print(list_samples_index_tr.head(25))

y_train = list_samples_index_tr
y_test = list_samples_index_ts

display(test_data.head(3))

display(train_data.head(4))

def get_resnet_model():
  def residual_block(X, kernels, stride):
    out = keras.layers.Conv1D(kernels, kernel_size, stride, padding='same')(X)
    out = keras.layers.BatchNormalization()(out)
    out = keras.layers.ReLU()(out)
    out = keras.layers.Conv1D(kernels, kernel_size, stride, padding='same')(out)
    out = keras.layers.BatchNormalization()(out)
    out = keras.layers.add([X, out])
    out = keras.layers.ReLU()(out)
    out = keras.layers.MaxPool1D(pool_size=2, strides=2, padding='same')(out)
    return out

  kernels = 256
  stride = 1
  kernel_size= 16

  inputs = keras.layers.Input([1023,1])
  X = keras.layers.Conv1D(kernels, stride)(inputs)
  X = residual_block(X, kernels, stride)
  X = residual_block(X, kernels, stride)
  X = residual_block(X, kernels, stride)
  X = residual_block(X, kernels, stride)
  X = residual_block(X, kernels, stride)
  X = residual_block(X, kernels, stride)
  X = residual_block(X, kernels, stride)


  X = keras.layers.Flatten()(X)
  X = keras.layers.Dense(16, activation='relu')(X)
  X = keras.layers.Dense(16, activation='relu')(X)
  output = keras.layers.Dense(1)(X)

  model = keras.Model(inputs=inputs, outputs=output)
  return model

#optimizer = keras.optimizers.Adam(learning_rate=0.001)
#model = get_resnet_model() 
#model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['mse'])

#lr_finder = LRFinder(start_lr=1e-7, end_lr= 1e-3, max_steps=100, smoothing=0.6)

#_ = model.fit(train_data, y_train, batch_size=256, epochs=5, callbacks=[lr_finder], verbose=False)
#lr_finder.plot()

resnet_model = get_resnet_model() 
resnet_model.summary()

#resnet_model.load_weights(filepath="weights_quant.hdf5")



# Set cyclical learning rate
N = train_data.shape[0]
batch_size = 32
iterations = N/batch_size
step_size= 2 * iterations

lr_schedule = CyclicalLearningRate(1e-6, 1e-3, step_size=step_size, scale_fn=lambda x: tf.pow(0.95,x))
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

save_best_weights = ModelCheckpoint(filepath="/content/drive/MyDrive/datasets/UVIGO/08_10_22_weights_quant.hdf5", verbose=0, save_best_only=True)

resnet_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['MSE'])
history = resnet_model.fit(train_data, y_train, validation_data=(test_data, y_test), shuffle=True, batch_size=batch_size, epochs=5000, callbacks=[save_best_weights])
#history = resnet_model.fit(train_data, y_train, shuffle=True, batch_size=batch_size, epochs=100)

pretty_plot(history, 'loss', lambda x: np.argmin(x))

display(test_data.iloc[:,:])
pred_test = resnet_model.predict(test_data.iloc[:,:])
pred = pd.DataFrame(pred_test,index=test_data.index, columns=['predicted'])
pred['real'] = y_test 
display(pred)

#display(train_data.iloc[:,:])
pred_train = resnet_model.predict(train_data.iloc[:,:])
pred_tr = pd.DataFrame(pred_train,index=train_data.index, columns=['predicted'])
pred_tr['real'] = y_train 
display(pred_tr)

resnet_model.evaluate(train_data, y_train)

Xnew = train_data
# make a prediction
ynew = resnet_model.predict(Xnew)
# show the inputs and predicted outputs
#print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))
display(ynew)
test_data

# Set cyclical learning rate
N = train_data.shape[0]
batch_size = 23
iterations = N/batch_size
step_size= 2 * iterations

lr_schedule = CyclicalLearningRate(1e-6, 1e-3, step_size=step_size, scale_fn=lambda x: tf.pow(0.95,x))
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)


dense_model = Sequential() # We are using the Sequential model because our network consists of a linear stack of layers

dense_model.add(Dense(train_data.shape[1], activation='relu', input_dim=train_data.shape[1])) # input layer which specifies the activation function and the number of input dimensions
dense_model.add(Dense(2048, activation='relu'))
dense_model.add(Dense(2048, activation='relu'))
dense_model.add(Dense(4096, activation='relu'))
dense_model.add(Dense(4096, activation='relu'))
dense_model.add(Dense(2048, activation='relu'))
dense_model.add(Dense(2048, activation='relu'))
dense_model.add(Dense(1024, activation='relu'))
dense_model.add(Dense(1024, activation='relu'))
dense_model.add(Dense(512, activation='relu'))
dense_model.add(Dense(512, activation='relu'))
dense_model.add(Dense(256, activation='relu'))
dense_model.add(Dense(256, activation='relu'))
dense_model.add(Dense(128, activation='relu'))
dense_model.add(Dense(128, activation='relu'))
dense_model.add(Dense(64, activation='relu'))
dense_model.add(Dense(64, activation='relu'))
dense_model.add(Dense(32, activation='relu'))
dense_model.add(Dense(32, activation='relu'))
dense_model.add(Dense(16, activation='relu'))
dense_model.add(Dense(16, activation='relu'))

dense_model.add(Dense(1)) # three nodes because there are two output classes 0(VIO), 1(DV) and 2(PDV)

# Compile the model
dense_model.compile(optimizer=optimizer, 
              loss='mean_squared_error', 
              metrics=['mse'])

history = dense_model.fit(train_data, y_train, shuffle=True, batch_size=batch_size, epochs=800)

Xnew = test_data
# make a prediction
ynew = dense_model.predict(Xnew)
# show the inputs and predicted outputs
#print("X=%s, Predicted=%s" % (Xnew[0], ynew[0]))
display(ynew)
test_data



